{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davis/VSCode/northern-lights/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "from tavily import TavilyClient\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown, JSON\n",
    "import google.generativeai as genai\n",
    "import urllib.parse\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Replace these with your actual keys or ensure they are in your .env file\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\") \n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "if not GEMINI_API_KEY or not TAVILY_API_KEY:\n",
    "    print(\"‚ö†Ô∏è WARNING: Please set GEMINI_API_KEY and TAVILY_API_KEY in your environment or this cell.\")\n",
    "\n",
    "# Configure Clients\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "#TODO experiment with different models\n",
    "model = genai.GenerativeModel('gemini-2.5-pro') \n",
    "tavily = TavilyClient(api_key=TAVILY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- BOLAGSVERKET AUTHENTICATION & API CAllS ---\n",
    "## Not needed atm \n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "CLIENT_ID = os.getenv(\"BOLAGSVERKET_CLIENT_ID\", \"YOUR_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"BOLAGSVERKET_CLIENT_SECRET\", \"YOUR_CLIENT_SECRET\")\n",
    "\n",
    "# Endpoints documented by Bolagsverket\n",
    "TOKEN_URL = \"https://portal.api.bolagsverket.se/oauth2/token\"\n",
    "# Base URL found in documentation for V√§rdefulla datam√§ngder\n",
    "API_BASE_URL = \"https://gw.api.bolagsverket.se/vardefulla-datamangder/v1\"\n",
    "\n",
    "def get_access_token():\n",
    "    \"\"\"Authenticates with Bolagsverket and returns an access token.\"\"\"\n",
    "    \n",
    "    # Encode client_id:client_secret in base64 for Basic Auth header\n",
    "    creds = f\"{CLIENT_ID}:{CLIENT_SECRET}\"\n",
    "    creds_b64 = base64.b64encode(creds.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Basic {creds_b64}\",\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        # needed for read / ping access \n",
    "        \"scope\": \"vardefulla-datamangder:read vardefulla-datamangder:ping\" \n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(TOKEN_URL, headers=headers, data=data)\n",
    "        response.raise_for_status()\n",
    "        token_data = response.json()\n",
    "        return token_data[\"access_token\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Error fetching token: {e}\")\n",
    "        if response.content:\n",
    "            print(f\"Details: {response.content}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def search_company(org_number, token):\n",
    "    \"\"\"\n",
    "    Fetches company information using the provided organization number and access token.\n",
    "    Args:\n",
    "        org_number (str or int): The 10-digit tax number of the organization. If the input \n",
    "            is not 10 digits, it will be zero-padded to ensure the correct format.\n",
    "        token (str): The access token required for authentication.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the company information retrieved from the API, \n",
    "            if the request is successful.\n",
    "        None: If the request fails, returns None and logs the error details.\n",
    "    Notes:\n",
    "        - The organization number should be provided without any dashes (e.g., \"5560160680\").\n",
    "        - Ensure that the `API_BASE_URL` variable is defined and points to the correct API endpoint.\n",
    "        - The function prints detailed error messages to help debug issues with the API response.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = f\"{API_BASE_URL}/organisationer\"\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # Use tax id of comapny -> without\n",
    "    # ---------------------------------------------------------\n",
    "    payload = {\n",
    "        \"identitetsbeteckning\": org_number\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Error fetching data: {e}\")\n",
    "        # Print detailed error to debug schema issues\n",
    "        print(f\"Response: {response.text}\")\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(company_name: str, org_id: str = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Ask Gemini to generate optimized search queries based on the entity name.\n",
    "    \"\"\"\n",
    "    input_data = f\"\"\"\n",
    "    <user>\n",
    "        <input>\n",
    "            <company_info>\n",
    "                <name>{company_name}</name>\n",
    "                <company_id>{org_id or \"Unknown\"}</company_id>\n",
    "            </company_info>\n",
    "        </input>\n",
    "    </user>\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(\"../app/services/prompts/query_internet.xml\", \"r\") as file:\n",
    "        QUERY_INTERNET_PROMPT = file.read()\n",
    "    full_prompt = QUERY_INTERNET_PROMPT + input_data\n",
    "    \n",
    "    response = model.generate_content(\n",
    "        full_prompt, \n",
    "        generation_config={\"response_mime_type\": \"application/json\"}\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        data = json.loads(response.text)\n",
    "        # Extract just the query strings from the structured objects\n",
    "        queries = [q[\"query\"] for q in data.get(\"queries\", [])]\n",
    "        # Deduplicate\n",
    "        return list(set(queries))\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing Gemini response: {e}\")\n",
    "        return [f\"{company_name} official website\", f\"{company_name} Sweden annual report\"]\n",
    "\n",
    "def perform_search(queries: list[str], query_number: int = 20):\n",
    "    \"\"\"\n",
    "    Step 2: Execute search using Tavily API.\n",
    "    \"\"\"\n",
    "    aggregated_context = []\n",
    "    \n",
    "    print(f\"üîé Executing {query_number} out of {len(queries)} search queries...\")\n",
    "    \n",
    "    # To save tokens/API credits, we might limit queries here\n",
    "    for query in queries[:query_number]: # Limit to top k queries for this demo\n",
    "        try:\n",
    "            print(f\"   -> Searching: '{query}'\")\n",
    "            # Tavily 'search' returns structured results with content\n",
    "            response = tavily.search(query=query, search_depth=\"advanced\", max_results=5)\n",
    "            \n",
    "            for result in response.get(\"results\", []):\n",
    "                aggregated_context.append(f\"Source: {result['url']}\\nContent: {result['content']}\\n---\")\n",
    "        except Exception as e:\n",
    "            print(f\"   x Error searching '{query}': {e}\")\n",
    "            \n",
    "    return \"\\n\".join(aggregated_context)\n",
    "\n",
    "def structure_data(company_name: str, search_context: str):\n",
    "    \"\"\"\n",
    "    Step 3: Structure the gathered raw text into the Northern Lights JSON schema.\n",
    "    \"\"\"\n",
    "    input_data = f\"\"\"\n",
    "    <user>\n",
    "        <input>\n",
    "            <source_data>\n",
    "                <bolagsverket>\n",
    "                    Legal Name: {company_name}\n",
    "                    Registered: Sweden\n",
    "                </bolagsverket>\n",
    "                <web_search>\n",
    "                    {search_context}\n",
    "                </web_search>\n",
    "            </source_data>\n",
    "            \n",
    "            <entity_context>\n",
    "                <entity_name>{company_name}</entity_name>\n",
    "                <entity_type>company</entity_type>\n",
    "            </entity_context>\n",
    "        </input>\n",
    "    </user>\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(\"../app/services/prompts/structure_data.xml\", \"r\") as file:\n",
    "        STRUCTURE_DATA_PROMPT = file.read()\n",
    "    full_prompt = STRUCTURE_DATA_PROMPT + input_data\n",
    "    #print(full_prompt)\n",
    "    response = model.generate_content(\n",
    "        full_prompt, \n",
    "        generation_config={\"response_mime_type\": \"application/json\"}\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        return json.loads(response.text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error structuring data: {e}\")\n",
    "        return {\"error\": response.text}\n",
    "    \n",
    "\n",
    "\n",
    "def generate_queries_funds(fund_name: str, org_id: str = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Ask Gemini to generate optimized search queries for a Fund/Investor.\n",
    "    Uses the 'query_investor_web.xml' prompt.\n",
    "    \"\"\"\n",
    "    # 1. Construct the Input XML strictly matching the prompt's expected structure\n",
    "    input_data = f\"\"\"\n",
    "    <user>\n",
    "        <input>\n",
    "            <investor_info>\n",
    "                <name>{fund_name}</name>\n",
    "                <company_id>{org_id or \"Unknown\"}</company_id>\n",
    "                <country_code>SE</country_code>\n",
    "            </investor_info>\n",
    "            \n",
    "            <missing_fields>\n",
    "                <field>description</field>\n",
    "                <field>investment_thesis</field>\n",
    "                <field>sectors</field>\n",
    "                <field>website</field>\n",
    "                <field>key_people</field>\n",
    "            </missing_fields>\n",
    "        </input>\n",
    "    </user>\n",
    "    \"\"\"\n",
    "    \n",
    "    # 2. Load the specific Investor Prompt\n",
    "    # Note: Adjust path if running from a different directory (e.g. '../src/app/...' from notebooks)\n",
    "    prompt_path = \"../app/services/prompts/query_internet_funds.xml\"\n",
    "    \n",
    "    try:\n",
    "        with open(prompt_path, \"r\") as file:\n",
    "            QUERY_INVESTOR_PROMPT = file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è Warning: Prompt file not found at {prompt_path}. Check your path.\")\n",
    "        return [f\"{fund_name} investment thesis\", f\"{fund_name} portfolio sectors\"]\n",
    "\n",
    "    full_prompt = QUERY_INVESTOR_PROMPT + input_data\n",
    "    \n",
    "    # 3. Call Gemini\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            full_prompt, \n",
    "            generation_config={\"response_mime_type\": \"application/json\"}\n",
    "        )\n",
    "        \n",
    "        # 4. Parse Response\n",
    "        data = json.loads(response.text)\n",
    "        \n",
    "        # Extract just the query strings\n",
    "        queries = [q[\"query\"] for q in data.get(\"queries\", [])]\n",
    "        \n",
    "        # Deduplicate and return\n",
    "        return list(set(queries))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating/parsing fund queries: {e}\")\n",
    "        # Fallback queries if LLM fails\n",
    "        return [\n",
    "            f\"{fund_name} investment thesis\", \n",
    "            f\"{fund_name} official website\", \n",
    "            f\"{fund_name} portfolio\"\n",
    "        ]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_data_fund(company_name: str, search_context: str, org_id: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Step 3: Structure the gathered raw text into the Northern Lights JSON schema.\n",
    "    Uses 'structure_data.xml' to extract fields like description, mission, key_people, etc.\n",
    "    \"\"\"\n",
    "    # 1. Define path to the generic company prompt\n",
    "    prompt_path = \"../app/services/prompts/structure_data_fund.xml\"\n",
    "    \n",
    "    # 2. Construct Input XML (matching the prompt's expected <user><input>... structure)\n",
    "    # We provide basic Bolagsverket info if we have it, otherwise just the name/ID context.\n",
    "    input_data = f\"\"\"\n",
    "    <user>\n",
    "        <input>\n",
    "            <source_data>\n",
    "                <bolagsverket>\n",
    "                    Legal Name: {company_name}\n",
    "                    Organization Number: {org_id or \"Unknown\"}\n",
    "                    Registered: Sweden\n",
    "                </bolagsverket>\n",
    "                <web_search>\n",
    "                    {search_context[:20000]} </web_search>\n",
    "            </source_data>\n",
    "            \n",
    "            <entity_context>\n",
    "                <entity_name>{company_name}</entity_name>\n",
    "                <entity_type>company</entity_type>\n",
    "                <known_org_number>{org_id or \"\"}</known_org_number>\n",
    "            </entity_context>\n",
    "        </input>\n",
    "    </user>\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 3. Load the Prompt File\n",
    "        with open(prompt_path, \"r\") as file:\n",
    "            STRUCTURE_DATA_PROMPT = file.read()\n",
    "            \n",
    "        # 4. Combine and Generate\n",
    "        full_prompt = STRUCTURE_DATA_PROMPT + input_data\n",
    "        \n",
    "        response = model.generate_content(\n",
    "            full_prompt, \n",
    "            generation_config={\"response_mime_type\": \"application/json\"}\n",
    "        )\n",
    "        \n",
    "        return json.loads(response.text)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: Prompt file not found at {prompt_path}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error structuring data for {company_name}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(company_name: str):\n",
    "    \"\"\"\n",
    "    Runs the data pipeline for the given company name.\n",
    "    \"\"\"\n",
    "    print(f\"Starting Scraper for: {company_name}\\n\")\n",
    "\n",
    "    # Generate Queries\n",
    "    queries = generate_queries(company_name)\n",
    "    print(\"‚úÖ Generated Queries:\")\n",
    "    print(json.dumps(queries[:2], indent=2))\n",
    "\n",
    "    # Scrape Web\n",
    "    if queries:\n",
    "        search_context = perform_search(queries, query_number=20)\n",
    "        print(f\"\\n‚úÖ Retrieved {len(search_context)} characters of context.\")\n",
    "    else:\n",
    "        search_context = \"\"\n",
    "        print(\"‚ùå No queries generated.\")\n",
    "\n",
    "    # Structure Data\n",
    "    if search_context:\n",
    "        print(\"\\nüß† Structuring data with Gemini...\")\n",
    "        structured_data = structure_data(company_name, search_context)\n",
    "        \n",
    "        print(\"\\n‚ú® FINAL JSON OUTPUT:\")\n",
    "        display(JSON(structured_data))\n",
    "    else:\n",
    "        print(\"‚ùå Skipping structuring due to lack of context.\")\n",
    "    return structured_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BeautifulSoup4 not available - Allabolag scraping will be limited\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import logging\n",
    "import uuid\n",
    "from typing import Any, Dict\n",
    "# 1. Setup path to allow imports from 'src'\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from app.services.portfolio_ingestion import ingest_company_with_portfolio\n",
    "from app.services.portfolio_ingestion import lookup_org_number_from_web\n",
    "from app.db.queries.relationship_queries import add_ownership\n",
    "\n",
    "import re\n",
    "# 1. Setup path to allow imports from 'src'\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from app.db.neo4j_client import get_driver\n",
    "# Import the query functions\n",
    "from app.db.queries.company_queries import upsert_company\n",
    "from app.db.queries.investor_queries import upsert_investor\n",
    "from app.db.queries.relationship_queries import add_ownership\n",
    "\n",
    "# =============================================================================\n",
    "# Helper: ID Formatting (Safe for UUIDs)\n",
    "# =============================================================================\n",
    "def format_org_id(org_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensures Swedish organization numbers are formatted as xxxxxx-xxxx.\n",
    "    SAFEGUARD: If the ID is a UUID (longer than 20 chars) or doesn't match \n",
    "    the 10-digit pattern, it returns the original value unchanged.\n",
    "    \"\"\"\n",
    "    if not org_id:\n",
    "        return org_id\n",
    "    \n",
    "    s_id = str(org_id)\n",
    "    \n",
    "    # 1. Safety Check: UUIDs are 36 chars. Swedish Org IDs are max 13 chars.\n",
    "    if len(s_id) > 20:\n",
    "        return s_id\n",
    "    \n",
    "    # 2. Clean: Remove non-digits to check raw length\n",
    "    clean_id = re.sub(r'\\D', '', s_id)\n",
    "    \n",
    "    # 3. Format: Only applying if strictly 10 digits\n",
    "    if len(clean_id) == 10:\n",
    "        return f\"{clean_id[:6]}-{clean_id[6:]}\"\n",
    "    \n",
    "    # Return original otherwise\n",
    "    return s_id\n",
    "\n",
    "# =============================================================================\n",
    "# Main Ingestion Logic\n",
    "# =============================================================================\n",
    "\n",
    "def ingest_company_full(data: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Ingests a company and its shareholders (as Investors/Funds), \n",
    "    and links them using the add_ownership method.\n",
    "    \"\"\"\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 1. Prepare Main Company Data\n",
    "    # -------------------------------------------------------\n",
    "    raw_id = data.get(\"organization_id\")\n",
    "    if not raw_id:\n",
    "        print(f\"Skipping {data.get('name')}: No organization_id\")\n",
    "        return\n",
    "        \n",
    "    # Apply standard formatting (Safe for UUIDs)\n",
    "    company_id = format_org_id(raw_id)\n",
    "\n",
    "    company_data = {\n",
    "        \"company_id\": company_id,\n",
    "        \"name\": data.get(\"name\"),\n",
    "        \"country_code\": data.get(\"country_code\", \"SE\"),\n",
    "        \"description\": data.get(\"description\", \"\"),\n",
    "        \"mission\": data.get(\"mission\", \"\"),\n",
    "        \"year_founded\": data.get(\"year_founded\"),\n",
    "        \"num_employees\": data.get(\"num_employees\"),\n",
    "        \"website\": data.get(\"website\", \"\"),\n",
    "        \"sectors\": data.get(\"sectors\", []),\n",
    "        \"aliases\": data.get(\"aliases\", []),\n",
    "        \"key_people\": data.get(\"key_people\", []),\n",
    "    }\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2. Upsert Main Company\n",
    "    # -------------------------------------------------------\n",
    "    try:\n",
    "        upsert_company(company_data)\n",
    "        print(f\"‚úÖ Upserted Company: {company_data['name']} ({company_id})\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error upserting company: {e}\")\n",
    "        return\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 3. Process Shareholders (Shareholder OWNS Company)\n",
    "    # -------------------------------------------------------\n",
    "    shareholder_items = data.get(\"shareholders\", [])\n",
    "    if shareholder_items:\n",
    "        print(f\"   Processing {len(shareholder_items)} shareholders...\")\n",
    "        for item in shareholder_items:\n",
    "            process_related_entity(item, target_company_id=company_id) \n",
    "\n",
    "def process_related_entity(\n",
    "    item: Dict[str, Any], \n",
    "    target_company_id: str, \n",
    "    relationship: str = \"shareholder\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Merged Logic Processor for Related Entities (Shareholders/Investors).\n",
    "    \"\"\"\n",
    "    name = item.get(\"name\")\n",
    "    if not name:\n",
    "        return None\n",
    "\n",
    "    # Use print or logger consistently; referencing logger here for safety\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # --- Step A: Initial Setup & Agentic Enrichment ---\n",
    "    # (Assuming generate_queries_funds, perform_search, structure_data_fund are defined globally or imported)\n",
    "    \n",
    "    initial_id = item.get(\"entity_id\")\n",
    "    if initial_id:\n",
    "        initial_id = format_org_id(initial_id)\n",
    "        \n",
    "    investor_data = {\n",
    "        \"company_id\": initial_id,\n",
    "        \"name\": name,\n",
    "        \"country_code\": \"SE\",\n",
    "        \"description\": f\"Ingested as {relationship} of {target_company_id}\",\n",
    "        \"sectors\": [],\n",
    "        \"website\": \"\",\n",
    "        \"key_people\": [],\n",
    "        \"investment_thesis\": \"\"\n",
    "    }\n",
    "\n",
    "    found_org_id = None\n",
    "\n",
    "    # ... [Enrichment Logic remains the same] ...\n",
    "    try:\n",
    "        # Placeholder for your enrichment functions (ensure these are imported!)\n",
    "        if 'generate_queries_funds' in globals():\n",
    "            queries = generate_queries_funds(name)\n",
    "            if queries:\n",
    "                raw_results = perform_search(queries, 20)\n",
    "                if raw_results:\n",
    "                    enriched_info = structure_data_fund(raw_results, name, initial_id)\n",
    "                    if enriched_info:\n",
    "                        clean_enriched = {k: v for k, v in enriched_info.items() if v}\n",
    "                        investor_data.update(clean_enriched)\n",
    "                        detected_id = clean_enriched.get(\"organization_id\")\n",
    "                        if detected_id:\n",
    "                            formatted_id = format_org_id(detected_id)\n",
    "                            # Basic check for Swedish ID length (digits only)\n",
    "                            clean_digits = re.sub(r'\\D', '', formatted_id)\n",
    "                            if len(clean_digits) == 10:\n",
    "                                found_org_id = formatted_id\n",
    "                                print(f\"      üîç Agents found official Swedish ID for {name}: {found_org_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ö†Ô∏è Enrichment pipeline error for {name}: {e}\")\n",
    "\n",
    "    # --- Step B: Ingestion Branching ---\n",
    "\n",
    "    final_entity_id = None\n",
    "\n",
    "    # PATH 1: Valid Swedish Org ID Found -> Full Viral Ingestion\n",
    "    if found_org_id:\n",
    "        # Set the final ID immediately, so we don't lose it if viral ingestion fails\n",
    "        final_entity_id = found_org_id\n",
    "        investor_data[\"company_id\"] = found_org_id\n",
    "\n",
    "        # 1. Try Viral Ingestion (BONUS STEP)\n",
    "        try:\n",
    "            print(f\"      üöÄ Valid ID found ({found_org_id}). Attempting viral ingestion...\")\n",
    "            # We assume this function is imported correctly\n",
    "            ingest_company_with_portfolio(found_org_id, investor_data[\"name\"])\n",
    "        except Exception as e:\n",
    "            # FIX: If viral ingestion fails, LOG IT but DO NOT RETURN. Continue to link!\n",
    "            logger.warning(f\"      ‚ö†Ô∏è Viral ingestion failed for {name} (Non-fatal): {e}\")\n",
    "\n",
    "        # 2. Upsert the Investor Node (ESSENTIAL STEP)\n",
    "        try:\n",
    "            # Ensure enriched data (sectors, etc.) is saved, overwriting/merging with whatever ingest_company did\n",
    "            upsert_investor(investor_data)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"      ‚ùå Critical: Failed to upsert investor node {name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # PATH 2: No Official ID -> Simple Insert with UUID\n",
    "    else:\n",
    "        print(f\"      üìâ No official Swedish ID found for {name}. Falling back to simple upsert.\")\n",
    "        \n",
    "        if not investor_data.get(\"company_id\"):\n",
    "            investor_data[\"company_id\"] = str(uuid.uuid5(uuid.NAMESPACE_DNS, name))\n",
    "        \n",
    "        final_entity_id = investor_data[\"company_id\"]\n",
    "        \n",
    "        try:\n",
    "            upsert_investor(investor_data)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"      ‚ùå Failed to simple upsert '{name}': {e}\")\n",
    "            return None\n",
    "\n",
    "    # --- Step C: Linking ---\n",
    "    # This will now run even if Viral Ingestion crashed\n",
    "    if final_entity_id:\n",
    "        try:\n",
    "            props = {\"source\": \"llm_ingest\"}\n",
    "            if \"ownership_pct\" in item and item[\"ownership_pct\"]:\n",
    "                 props[\"share_percentage\"] = float(item[\"ownership_pct\"])\n",
    "\n",
    "            if relationship == \"shareholder\":\n",
    "                add_ownership(\n",
    "                    owner_id=final_entity_id, \n",
    "                    company_id=target_company_id,\n",
    "                    properties=props\n",
    "                )\n",
    "                print(f\"      ‚Ü≥ Linked Shareholder: {name} ({final_entity_id}) -> {target_company_id}\")\n",
    "            \n",
    "            return final_entity_id\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ùå Failed to link '{name}': {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Scraper for: IKEA\n",
      "\n",
      "‚úÖ Generated Queries:\n",
      "[\n",
      "  \"IKEA bransch och verksamhetsomr\\u00e5den\",\n",
      "  \"IKEA Wikipedia company profile\"\n",
      "]\n",
      "üîé Executing 20 out of 23 search queries...\n",
      "   -> Searching: 'IKEA bransch och verksamhetsomr√•den'\n",
      "   -> Searching: 'IKEA Wikipedia company profile'\n",
      "   -> Searching: 'IKEA \"our vision is to create a better everyday life\"'\n",
      "   -> Searching: 'IKEA CEO board of directors leadership'\n",
      "   -> Searching: 'IKEA ownership structure Stichting INGKA Foundation'\n",
      "   -> Searching: '\"IKEA of Sweden AB\" organisationsnummer'\n",
      "   -> Searching: 'IKEA mission statement and vision'\n",
      "   -> Searching: 'IKEA officiell webbplats Sverige'\n",
      "   -> Searching: '\"Inter IKEA Group\" management team'\n",
      "   -> Searching: 'IKEA holding company name \"Inter IKEA Group\"'\n",
      "   -> Searching: 'IKEA number of employees worldwide'\n",
      "   -> Searching: 'IKEA official website corporate information'\n",
      "   -> Searching: 'about IKEA company description'\n",
      "   -> Searching: 'vad √§r IKEA f√∂retagsprofil'\n",
      "   -> Searching: 'IKEA LinkedIn number of employees'\n",
      "   -> Searching: 'what industry is IKEA in'\n",
      "   -> Searching: 'IKEA VD och koncernledning'\n",
      "   -> Searching: 'IKEA v√§rderingar och vision'\n",
      "   -> Searching: 'IKEA antal anst√§llda globalt'\n",
      "   -> Searching: 'IKEA industry sector business area'\n",
      "\n",
      "‚úÖ Retrieved 114310 characters of context.\n",
      "\n",
      "üß† Structuring data with Gemini...\n",
      "\n",
      "‚ú® FINAL JSON OUTPUT:\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "aliases": [
        "IKEA",
        "IKEA Group",
        "Ingka Group",
        "Inter IKEA Group"
       ],
       "country_code": "SE",
       "customers": [],
       "description": "IKEA is a multinational conglomerate founded in Sweden that designs and sells ready-to-assemble furniture, home accessories, and other goods. The company is known for its modernist, functional designs and its business model which focuses on affordability and customer assembly.",
       "key_people": [
        "Ingvar Kamprad",
        "Jesper Brodin",
        "Juvencio Maeztu",
        "Jon Abrahamsson Ring",
        "Jakub Jankowski",
        "Lars-Johan Jarnheimer",
        "Fredrika Inger"
       ],
       "mission": "To offer a wide range of well-designed, functional home furnishing products at prices so low that as many people as possible will be able to afford them.",
       "name": "IKEA of Sweden AB",
       "num_employees": 170000,
       "num_shares": null,
       "organization_id": "556074-7551",
       "portfolio": [
        {
         "entity_id": null,
         "entity_type": "company",
         "name": "Ingka Holding B.V."
        },
        {
         "entity_id": null,
         "entity_type": "company",
         "name": "Inter IKEA Holding B.V."
        },
        {
         "entity_id": null,
         "entity_type": "company",
         "name": "IKEA Industry AB"
        },
        {
         "entity_id": null,
         "entity_type": "company",
         "name": "IKEA Supply AG"
        },
        {
         "entity_id": null,
         "entity_type": "company",
         "name": "Inter IKEA Systems B.V."
        }
       ],
       "sectors": [
        "Retail",
        "Furniture",
        "Home Furnishings",
        "Home Electronics",
        "Food Products"
       ],
       "shareholders": [
        {
         "entity_id": null,
         "entity_type": "fund",
         "name": "Stichting Ingka Foundation"
        },
        {
         "entity_id": null,
         "entity_type": "fund",
         "name": "Interogo Foundation"
        }
       ],
       "website": "https://www.ikea.com",
       "year_founded": "1943"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Upserted Company: IKEA of Sweden AB (556074-7551)\n",
      "   Processing 2 shareholders...\n",
      "üîé Executing 20 out of 16 search queries...\n",
      "   -> Searching: 'Stichting Ingka Foundation'\n",
      "   -> Searching: 'Ingka Foundation key people LinkedIn'\n",
      "   -> Searching: 'Stichting Ingka Foundation purpose overview'\n",
      "   -> Searching: 'Stichting Ingka Foundation leadership team'\n",
      "   -> Searching: 'Stichting Ingka Foundation portfolio focus areas'\n",
      "   -> Searching: 'Ingka Foundation areas of interest grants'\n",
      "   -> Searching: 'Stichting Ingka Foundation about us mission'\n",
      "   -> Searching: 'Stichting Ingka Foundation investment approach'\n",
      "   -> Searching: 'Ingka Foundation contact information'\n",
      "   -> Searching: 'site:ingka.com \"Ingka Foundation\"'\n",
      "   -> Searching: 'Ingka Foundation philanthropic funding criteria'\n",
      "   -> Searching: 'Stichting Ingka Foundation board members'\n",
      "   -> Searching: 'What is Stichting Ingka Foundation?'\n",
      "   -> Searching: 'Stichting Ingka Foundation key partnerships sectors'\n",
      "   -> Searching: 'Stichting Ingka Foundation official website'\n",
      "   -> Searching: 'Stichting Ingka Foundation investment strategy'\n",
      "      üìâ No official Swedish ID found for Stichting Ingka Foundation. Falling back to simple upsert.\n",
      "      ‚Ü≥ Linked Shareholder: Stichting Ingka Foundation (eb2e5723-90ea-5a7e-b79a-d5d2c7f46918) -> 556074-7551\n",
      "üîé Executing 20 out of 16 search queries...\n",
      "   -> Searching: 'Interogo Foundation contact information'\n",
      "   -> Searching: 'Interogo Foundation LinkedIn profile summary'\n",
      "   -> Searching: 'Interogo Foundation official website'\n",
      "   -> Searching: 'Interogo Holding portfolio sectors'\n",
      "   -> Searching: 'Interogo Foundation'\n",
      "   -> Searching: 'Interogo Foundation investment purpose mission'\n",
      "   -> Searching: 'Interogo Foundation management team LinkedIn'\n",
      "   -> Searching: 'Interogo Holding investment criteria'\n",
      "   -> Searching: 'Interogo Foundation about us overview'\n",
      "   -> Searching: 'Interogo Foundation board of directors'\n",
      "   -> Searching: 'Interogo Holding AG investments by industry'\n",
      "   -> Searching: 'Interogo Foundation investment strategy'\n",
      "   -> Searching: 'Interogo Foundation Chairman'\n",
      "   -> Searching: 'What is Interogo Foundation'\n",
      "   -> Searching: 'Interogo Foundation investment focus areas'\n",
      "   -> Searching: 'Interogo Foundation Crunchbase'\n",
      "      üìâ No official Swedish ID found for Interogo Foundation. Falling back to simple upsert.\n",
      "      ‚Ü≥ Linked Shareholder: Interogo Foundation (0fcda747-6ff1-59a7-9d24-e8e83771b895) -> 556074-7551\n",
      "Starting Scraper for: H&M (Hennes & Mauritz)\n",
      "\n",
      "‚úÖ Generated Queries:\n",
      "[\n",
      "  \"Hennes & Mauritz Wikipedia\",\n",
      "  \"H&M Hennes & Mauritz AB officiell hemsida\"\n",
      "]\n",
      "üîé Executing 20 out of 21 search queries...\n",
      "   -> Searching: 'Hennes & Mauritz Wikipedia'\n",
      "   -> Searching: 'H&M Hennes & Mauritz AB officiell hemsida'\n",
      "   -> Searching: 'H&M Hennes & Mauritz LinkedIn employees'\n",
      "   -> Searching: 'H&M Hennes & Mauritz AB official website'\n",
      "   -> Searching: 'Hennes & Mauritz AB st√∂rsta √§gare'\n",
      "   -> Searching: 'Hennes & Mauritz AB \"styrelse och ledning\"'\n",
      "   -> Searching: 'H&M Hennes & Mauritz AB organisationsnummer'\n",
      "   -> Searching: 'Hennes & Mauritz company registration number Sweden'\n",
      "   -> Searching: 'H&M leadership team'\n",
      "   -> Searching: 'Hennes & Mauritz anst√§llda antal'\n",
      "   -> Searching: 'H&M Hennes & Mauritz AB major shareholders'\n",
      "   -> Searching: 'H&M Hennes & Mauritz number of employees'\n",
      "   -> Searching: 'H&M Hennes & Mauritz mission statement'\n",
      "   -> Searching: 'H&M Hennes & Mauritz industry sector'\n",
      "   -> Searching: 'H&M group brands'\n",
      "   -> Searching: 'Hennes & Mauritz AB \"aff√§rsid√© och vision\"'\n",
      "   -> Searching: 'H&M bransch kategori'\n",
      "   -> Searching: 'Vad √§r H&M Hennes & Mauritz'\n",
      "   -> Searching: 'H&M Hennes & Mauritz CEO board of directors'\n",
      "   -> Searching: 'H&M target customer demographics'\n",
      "\n",
      "‚úÖ Retrieved 113559 characters of context.\n",
      "\n",
      "üß† Structuring data with Gemini...\n",
      "\n",
      "‚ú® FINAL JSON OUTPUT:\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "aliases": [
        "H&M",
        "Hennes & Mauritz",
        "H and M",
        "H&M Group"
       ],
       "country_code": "SE",
       "customers": [],
       "description": "H & M Hennes & Mauritz AB, commonly known as H&M, is a Swedish multinational fast-fashion retailer headquartered in Stockholm. Known for its fast-fashion business model, H&M sells clothing, accessories, and homeware. The company has a significant global presence, operating thousands of stores across numerous geographical markets.",
       "key_people": [
        "Erling Persson",
        "Stefan Persson",
        "Karl-Johan Persson",
        "Daniel Erv√©r"
       ],
       "mission": "to offer fashion and quality at the best price.",
       "name": "H & M Hennes & Mauritz AB",
       "num_employees": 140000,
       "num_shares": 1629686837,
       "organization_id": "556042-7220",
       "portfolio": [
        {
         "entity_id": "556070-1715",
         "entity_type": "company",
         "name": "H & M Hennes & Mauritz GBC AB"
        },
        {
         "entity_id": "556151-2376",
         "entity_type": "company",
         "name": "H & M Hennes & Mauritz Sverige AB"
        }
       ],
       "sectors": [
        "Retail",
        "Fashion",
        "Apparel",
        "Homeware",
        "Cosmetics"
       ],
       "shareholders": [
        {
         "entity_id": null,
         "entity_type": "company",
         "name": "Ramsbury Invest AB"
        },
        {
         "entity_id": null,
         "entity_type": "fund",
         "name": "Lottie Tham"
        },
        {
         "entity_id": null,
         "entity_type": "fund",
         "name": "AMF Fonder AB"
        },
        {
         "entity_id": null,
         "entity_type": "fund",
         "name": "The Vanguard Group, Inc."
        },
        {
         "entity_id": null,
         "entity_type": "fund",
         "name": "Fj√§rde AP-fonden"
        }
       ],
       "website": "https://hmgroup.com",
       "year_founded": "1947"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Upserted Company: H & M Hennes & Mauritz AB (556042-7220)\n",
      "   Processing 5 shareholders...\n",
      "üîé Executing 20 out of 19 search queries...\n",
      "   -> Searching: '\"Ramsbury Invest AB\" investeringsstrategi'\n",
      "   -> Searching: 'contact \"Ramsbury Invest AB\"'\n",
      "   -> Searching: 'Ramsbury Invest AB LinkedIn profile overview'\n",
      "   -> Searching: '\"Ramsbury Invest AB\" about us'\n",
      "   -> Searching: 'who are the partners at \"Ramsbury Invest AB\" LinkedIn'\n",
      "   -> Searching: '\"Ramsbury Invest AB\" team partners'\n",
      "   -> Searching: 'Ramsbury Invest AB'\n",
      "   -> Searching: '\"Ramsbury Invest AB\" bolagsinformation'\n",
      "   -> Searching: 'Ramsbury Invest investment criteria ticket size'\n",
      "   -> Searching: '\"Ramsbury Invest AB\" investment thesis'\n",
      "   -> Searching: '\"Ramsbury Invest AB\" portfolio companies sectors'\n",
      "   -> Searching: '\"Ramsbury Invest AB\" styrelse VD'\n",
      "   -> Searching: 'Ramsbury Invest AB official website'\n",
      "   -> Searching: '\"Ramsbury Invest AB\" allabolag.se'\n",
      "   -> Searching: 'what sectors does Ramsbury Invest focus on'\n",
      "   -> Searching: '\"Ramsbury Invest AB\" branscher investeringar'\n",
      "   -> Searching: '\"Ramsbury Invest AB\" om oss verksamhet'\n",
      "   -> Searching: 'Ramsbury Invest AB Crunchbase'\n",
      "   -> Searching: '\"Ramsbury Invest AB\" organisationsnummer'\n",
      "      üîç Agents found official Swedish ID for Ramsbury Invest AB: 556423-5769\n",
      "      üöÄ Valid ID found (556423-5769). Attempting viral ingestion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error inside hack_net worker for 556423-5769: unindent does not match any outer indentation level (hack_net.py, line 523)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/davis/VSCode/northern-lights/app/services/portfolio_ingestion.py\", line 103, in _hack_net_worker\n",
      "    import hack_net\n",
      "  File \"/Users/davis/VSCode/northern-lights/data_pipeline/illegal/hack_net.py\", line 523\n",
      "    else:\n",
      "         ^\n",
      "IndentationError: unindent does not match any outer indentation level\n",
      "Error extracting portfolio from FI for 556423-5769: unindent does not match any outer indentation level (hack_net.py, line 523)\n",
      "No portfolio data extracted for 556423-5769\n",
      "Gemini returned non-dict JSON: <class 'list'>, converting to dict\n",
      "/Users/davis/VSCode/northern-lights/.venv/lib/python3.13/site-packages/neo4j/_sync/work/result.py:625: UserWarning: Expected a result with a single record, but found multiple.\n",
      "  warn(\n",
      "Skipping self-ownership: Stefan Persson (556423-5769)\n",
      "Skipping self-ownership: Karl-Johan Persson (556423-5769)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚Ü≥ Linked Shareholder: Ramsbury Invest AB (556423-5769) -> 556042-7220\n",
      "üîé Executing 20 out of 19 search queries...\n",
      "   -> Searching: 'Lottie Tham Dagens Industri investments focus'\n",
      "   -> Searching: 'Lottie Tham investment criteria'\n",
      "   -> Searching: 'Lottie Tham family office team'\n",
      "   -> Searching: 'Lottie Tham angel investment strategy'\n",
      "   -> Searching: 'Lottie Tham'\n",
      "   -> Searching: 'Lottie Tham holdingbolag namn'\n",
      "   -> Searching: 'Lottie Tham's investment company name'\n",
      "   -> Searching: 'Lottie Tham family office'\n",
      "   -> Searching: 'Lottie Tham H&M investments biography'\n",
      "   -> Searching: 'Pieter Tham investments'\n",
      "   -> Searching: 'what industries does Lottie Tham invest in'\n",
      "   -> Searching: 'Lottie Tham investor profile'\n",
      "   -> Searching: 'Vem √§r Lottie Tham investerare'\n",
      "   -> Searching: 'Lottie Tham bolagsengagemang'\n",
      "   -> Searching: 'Lottie Tham portfolio companies sectors'\n",
      "   -> Searching: 'Lottie Tham holdingbolag organisationsnummer'\n",
      "   -> Searching: 'Lottie Tham investment advisors'\n",
      "   -> Searching: 'Lottie Tham allabolag.se'\n",
      "   -> Searching: 'Lottie Tham innehav bolag'\n",
      "      üìâ No official Swedish ID found for Lottie Tham. Falling back to simple upsert.\n",
      "      ‚Ü≥ Linked Shareholder: Lottie Tham (ff6c88f5-cfe8-5644-94f0-7d0a4f29a85f) -> 556042-7220\n",
      "üîé Executing 20 out of 19 search queries...\n",
      "   -> Searching: 'AMF Fonder AB \"fund managers\" OR \"f√∂rvaltare\"'\n",
      "   -> Searching: 'AMF Fonder AB orgnr allabolag'\n",
      "   -> Searching: 'What is AMF Fonder AB'\n",
      "   -> Searching: 'AMF Fonder AB portfolio sectors'\n",
      "   -> Searching: 'AMF Fonder AB ledning'\n",
      "   -> Searching: 'AMF Fonder AB kontaktinformation'\n",
      "   -> Searching: 'AMF Fonder AB LinkedIn about'\n",
      "   -> Searching: 'AMF Fonder AB innehav branscher'\n",
      "   -> Searching: 'AMF Fonder AB Bolagsverket'\n",
      "   -> Searching: 'AMF Fonder AB CEO OR \"VD\"'\n",
      "   -> Searching: 'AMF Fonder AB official website'\n",
      "   -> Searching: 'AMF Fonder AB top holdings industries'\n",
      "   -> Searching: 'AMF Fonder AB \"om oss\"'\n",
      "   -> Searching: 'AMF Fonder AB investment strategy'\n",
      "   -> Searching: 'AMF Fonder AB'\n",
      "   -> Searching: 'AMF Fonder AB site:amf.se'\n",
      "   -> Searching: 'AMF Fonder AB annual report \"investment approach\"'\n",
      "   -> Searching: 'AMF Fonder AB organisationsnummer'\n",
      "   -> Searching: 'AMF Fonder AB investeringsfilosofi'\n",
      "      üîç Agents found official Swedish ID for AMF Fonder AB: 556549-2922\n",
      "      üöÄ Valid ID found (556549-2922). Attempting viral ingestion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error inside hack_net worker for 556549-2922: unindent does not match any outer indentation level (hack_net.py, line 523)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/davis/VSCode/northern-lights/app/services/portfolio_ingestion.py\", line 103, in _hack_net_worker\n",
      "    import hack_net\n",
      "  File \"/Users/davis/VSCode/northern-lights/data_pipeline/illegal/hack_net.py\", line 523\n",
      "    else:\n",
      "         ^\n",
      "IndentationError: unindent does not match any outer indentation level\n",
      "Error extracting portfolio from FI for 556549-2922: unindent does not match any outer indentation level (hack_net.py, line 523)\n",
      "No portfolio data extracted for 556549-2922\n",
      "Skipping self-ownership: AMF Tj√§nstepension AB (556549-2922)\n",
      "Skipping self-ownership: AMF (556549-2922)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚Ü≥ Linked Shareholder: AMF Fonder AB (556549-2922) -> 556042-7220\n",
      "üîé Executing 20 out of 16 search queries...\n",
      "   -> Searching: 'what industries does Vanguard's private equity arm invest in'\n",
      "   -> Searching: '\"Vanguard\" investment criteria for private markets'\n",
      "   -> Searching: 'The Vanguard Group, Inc.'\n",
      "   -> Searching: '\"Vanguard Group\" venture capital investment strategy'\n",
      "   -> Searching: '\"The Vanguard Group, Inc.\" LinkedIn company profile'\n",
      "   -> Searching: '\"The Vanguard Group, Inc.\" Crunchbase profile overview'\n",
      "   -> Searching: '\"Vanguard\" growth equity focus areas'\n",
      "   -> Searching: 'Vanguard corporate headquarters site'\n",
      "   -> Searching: '\"The Vanguard Group, Inc.\" leadership team'\n",
      "   -> Searching: 'Vanguard institutional investor portal'\n",
      "   -> Searching: '\"Vanguard Group\" private equity investment thesis'\n",
      "   -> Searching: '\"The Vanguard Group, Inc.\" official website'\n",
      "   -> Searching: '\"The Vanguard Group, Inc.\" about us summary'\n",
      "   -> Searching: '\"Vanguard Group\" private equity portfolio sectors'\n",
      "   -> Searching: '\"Vanguard Group\" head of private equity linkedin'\n",
      "   -> Searching: '\"Vanguard Group\" senior executives CEO CIO'\n",
      "      üìâ No official Swedish ID found for The Vanguard Group, Inc.. Falling back to simple upsert.\n",
      "      ‚Ü≥ Linked Shareholder: The Vanguard Group, Inc. (02c34e7b-8270-5c4a-aa52-e0dec86907a9) -> 556042-7220\n",
      "üîé Executing 20 out of 16 search queries...\n",
      "   -> Searching: 'Fj√§rde AP-fonden management team CEO'\n",
      "   -> Searching: 'Fj√§rde AP-fonden tillg√•ngsallokering'\n",
      "   -> Searching: 'Fj√§rde AP-fonden'\n",
      "   -> Searching: 'AP4 fund portfolio allocation by asset class'\n",
      "   -> Searching: 'Fj√§rde AP-fonden Crunchbase profile'\n",
      "   -> Searching: 'Fj√§rde AP-fonden st√∂rsta innehav'\n",
      "   -> Searching: 'Fj√§rde AP-fonden investeringsstrategi'\n",
      "   -> Searching: 'Fj√§rde AP-fonden ledning'\n",
      "   -> Searching: 'Fj√§rde AP-fonden om oss'\n",
      "   -> Searching: 'Fj√§rde AP-fonden LinkedIn \"about\"'\n",
      "   -> Searching: 'Fourth Swedish National Pension Fund AP4 summary'\n",
      "   -> Searching: 'Fj√§rde AP-fonden investment philosophy mandate'\n",
      "   -> Searching: 'AP4 annual report investment process'\n",
      "   -> Searching: 'AP4 fund board of directors'\n",
      "   -> Searching: 'Fj√§rde AP-fonden official website'\n",
      "   -> Searching: 'AP4 Fonden kontakt'\n",
      "      üìâ No official Swedish ID found for Fj√§rde AP-fonden. Falling back to simple upsert.\n",
      "      ‚Ü≥ Linked Shareholder: Fj√§rde AP-fonden (23ed45f2-ee8f-5ee0-8212-cb59e18d6e74) -> 556042-7220\n"
     ]
    }
   ],
   "source": [
    "### Run \n",
    "if __name__ == \"__main__\":\n",
    "    famous_swedish_companies = [\n",
    "        #\"Spotify AB\",\n",
    "        \"IKEA\",\n",
    "        #\"Volvo Group\",\n",
    "        \"H&M (Hennes & Mauritz)\",\n",
    "        #\"Ericsson\",\n",
    "        #\"Scania\",\n",
    "        #\"Electrolux\"\n",
    "    ]\n",
    "    \n",
    "    for company in famous_swedish_companies:\n",
    "        ingest_company_full(run_pipeline(company))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:19: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:19: SyntaxWarning: invalid escape sequence '\\W'\n",
      "/var/folders/7r/xtx89_9j54l4w8l78by0ycn80000gn/T/ipykernel_88584/3467345134.py:19: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  - \\W matches any character that is NOT alphanumeric (a-z, A-Z, 0-9).\n",
      "/var/folders/7r/xtx89_9j54l4w8l78by0ycn80000gn/T/ipykernel_88584/3467345134.py:106: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  with driver.session() as session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching nodes from Neo4j (Global Search)...\n",
      "Analyzing 65 nodes for duplicates...\n",
      "No node duplicates found.\n",
      "Starting Nuclear Edge Cleanup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7r/xtx89_9j54l4w8l78by0ycn80000gn/T/ipykernel_88584/3467345134.py:198: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  with driver.session() as session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Deleted 0 duplicate edges in this batch...\n",
      "Edge cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import networkx as nx\n",
    "from typing import List, Dict, Any\n",
    "from neo4j import GraphDatabase\n",
    "import google.generativeai as genai\n",
    "# ==========================================\n",
    "# 1. HELPERS\n",
    "# ==========================================\n",
    "# TODO fix duplicated node issues \n",
    "\n",
    "def clean_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    STRICT Cleaning (Case Insensitive):\n",
    "    1. Converts to LOWERCASE.\n",
    "    2. Removes ALL spaces, underscores, and special characters.\n",
    "    \n",
    "    Logic:\n",
    "    - \\W matches any character that is NOT alphanumeric (a-z, A-Z, 0-9).\n",
    "    - _ matches the underscore (which \\W usually includes as a \"word\" char, so we explicitly remove it).\n",
    "    \n",
    "    Examples:\n",
    "    - \"Lannebo Fonder\"      -> \"lannebofonder\"\n",
    "    - \"LANNEBO_FONDER\"      -> \"lannebofonder\"\n",
    "    - \"Lannebo-Fonder, AB\"  -> \"lannebofonderab\"\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    \n",
    "    # regex [\\W_] removes anything that is NOT a letter or number\n",
    "    cleaned = re.sub(r'[\\W_]+', '', str(name))\n",
    "    \n",
    "    return cleaned.lower()\n",
    "\n",
    "def generate_golden_record(model, node_properties_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Uses Gemini to merge conflicting properties into one JSON object.\n",
    "    \"\"\"\n",
    "    # 1. Sanitize inputs\n",
    "    sanitized_props = []\n",
    "    for prop in node_properties_list:\n",
    "        p_copy = prop.copy()\n",
    "        # Remove metadata not needed for merging context\n",
    "        if 'vector' in p_copy: del p_copy['vector']\n",
    "        if 'elementId' in p_copy: del p_copy['elementId']\n",
    "        sanitized_props.append(p_copy)\n",
    "\n",
    "    # 2. Fallback if model is not available\n",
    "    if not model:\n",
    "        print(\"Gemini model not initialized, using first record as fallback.\")\n",
    "        return sanitized_props[0]\n",
    "\n",
    "    # 3. Prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a Data Reconciliation Expert. \n",
    "    I have {len(sanitized_props)} records representing the SAME entity.\n",
    "    Your task: Merge them into one single \"Golden Record\" JSON object.\n",
    "    \n",
    "    Rules:\n",
    "    1. Name: Pick the most official/capitalized version.\n",
    "    2. Description: Combine if complementary.\n",
    "    3. Sectors: Merge into a unique list.\n",
    "    4. Return ONLY valid raw JSON. No Markdown.\n",
    "    \n",
    "    Input Records:\n",
    "    {json.dumps(sanitized_props, indent=2, default=str)}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        text = response.text.strip()\n",
    "        \n",
    "        # Robust Markdown Cleaning\n",
    "        if text.startswith(\"```\"):\n",
    "            lines = text.splitlines()\n",
    "            if len(lines) >= 3:\n",
    "                # Remove first (```json) and last (```) lines\n",
    "                text = \"\\n\".join(lines[1:-1])\n",
    "            else:\n",
    "                text = text.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        \n",
    "        return json.loads(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Gemini merge failed ({e}). Using first record as fallback.\")\n",
    "        return sanitized_props[0]\n",
    "\n",
    "# ==========================================\n",
    "# 2. CORE LOGIC\n",
    "# ==========================================\n",
    "\n",
    "def fetch_and_cluster_duplicates(driver) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Scans ALL nodes. Matches if:\n",
    "    1. Organization IDs are identical.\n",
    "    2. OR Names are identical (after strict, case-insensitive cleaning).\n",
    "    \"\"\"\n",
    "    print(\"Fetching nodes from Neo4j (Global Search)...\")\n",
    "    \n",
    "    # Matches ANY node that has a name OR a company_id\n",
    "    query = \"\"\"\n",
    "    MATCH (n) \n",
    "    WHERE n.name IS NOT NULL OR n.company_id IS NOT NULL\n",
    "    RETURN elementId(n) as node_id, n.company_id as org_id, n.name as name\n",
    "    \"\"\"\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        result = session.run(query)\n",
    "        nodes = list(result)\n",
    "\n",
    "    print(f\"Analyzing {len(nodes)} nodes for duplicates...\")\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    seen_org_ids = {} # org_id -> node_id\n",
    "    seen_names = {}   # clean_name -> node_id\n",
    "\n",
    "    for record in nodes:\n",
    "        current_node_id = record[\"node_id\"]\n",
    "        org_id = record[\"org_id\"]\n",
    "        raw_name = record[\"name\"]\n",
    "        \n",
    "        G.add_node(current_node_id)\n",
    "        \n",
    "        # A. ID Match\n",
    "        if org_id and str(org_id).strip(): \n",
    "            if org_id in seen_org_ids:\n",
    "                G.add_edge(current_node_id, seen_org_ids[org_id])\n",
    "            else:\n",
    "                seen_org_ids[org_id] = current_node_id\n",
    "\n",
    "        # B. Strict Case-Insensitive Name Match\n",
    "        if raw_name:\n",
    "            c_name = clean_name(raw_name)\n",
    "            if c_name: # Only process if name didn't reduce to empty string\n",
    "                if c_name in seen_names:\n",
    "                    G.add_edge(current_node_id, seen_names[c_name])\n",
    "                else:\n",
    "                    seen_names[c_name] = current_node_id\n",
    "\n",
    "    # Find connected groups larger than 1\n",
    "    clusters = [list(c) for c in nx.connected_components(G) if len(c) > 1]\n",
    "    return clusters\n",
    "\n",
    "def get_node_properties(driver, node_ids: List[str]) -> List[Dict]:\n",
    "    query = \"MATCH (n) WHERE elementId(n) IN $ids RETURN [n IN collect(n) | properties(n)] as props\"\n",
    "    with driver.session() as session:\n",
    "        record = session.run(query, ids=node_ids).single()\n",
    "        return record[\"props\"] if record else []\n",
    "\n",
    "def execute_merge(tx, node_ids: List[str], golden_record: Dict, master_vector: List[float]):\n",
    "    \"\"\"\n",
    "    Merges nodes using APOC and re-points their edges.\n",
    "    \"\"\"\n",
    "    # Note: apoc.refactor.mergeNodes requires a list of nodes, not IDs.\n",
    "    # We fetch them first inside the transaction query.\n",
    "    query = \"\"\"\n",
    "    MATCH (n) WHERE elementId(n) IN $node_ids\n",
    "    WITH collect(n) as nodes\n",
    "    CALL apoc.refactor.mergeNodes(nodes, {\n",
    "        properties: 'discard', \n",
    "        mergeRels: true,\n",
    "        produceSelfRef: false\n",
    "    })\n",
    "    YIELD node\n",
    "    SET node += $golden_props\n",
    "    \"\"\"\n",
    "    params = {\"node_ids\": node_ids, \"golden_props\": golden_record}\n",
    "    \n",
    "    # Only set vector if it exists and is not None\n",
    "    if master_vector:\n",
    "        query += \" SET node.vector = $vector\"\n",
    "        params[\"vector\"] = master_vector\n",
    "    \n",
    "    tx.run(query, params)\n",
    "\n",
    "def force_merge_edges_nuclear(batch_size=1000):\n",
    "    \"\"\"\n",
    "    The 'Nuclear' Edge Deduplication.\n",
    "    Identifies parallel edges (Same Start, Same End, Same Type).\n",
    "    Keeps the FIRST edge found, DELETES the rest.\n",
    "    \"\"\"\n",
    "    driver = get_driver()\n",
    "    print(\"Starting Nuclear Edge Cleanup...\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            query = \"\"\"\n",
    "            MATCH (s)-[r]->(e)\n",
    "            WITH s, e, type(r) AS t, collect(r) AS rels\n",
    "            WHERE size(rels) > 1\n",
    "            WITH rels LIMIT $batch_size\n",
    "            \n",
    "            // Keep head (first), delete tail (duplicates)\n",
    "            UNWIND tail(rels) as duplicate_edge\n",
    "            DELETE duplicate_edge\n",
    "            RETURN count(duplicate_edge) as deleted_count\n",
    "            \"\"\"\n",
    "            \n",
    "            with driver.session() as session:\n",
    "                result = session.run(query, batch_size=batch_size)\n",
    "                record = result.single()\n",
    "                if not record:\n",
    "                    break\n",
    "                    \n",
    "                deleted = record[\"deleted_count\"]\n",
    "                print(f\" -> Deleted {deleted} duplicate edges in this batch...\")\n",
    "                \n",
    "                if deleted == 0:\n",
    "                    break\n",
    "    finally:\n",
    "        print(\"Edge cleanup complete.\")\n",
    "        driver.close()\n",
    "\n",
    "def process_smart_deduplication():\n",
    "    driver = get_driver()\n",
    "    \n",
    "    try:\n",
    "        # 1. Cluster duplicates based on ID or Cleaned Name\n",
    "        clusters = fetch_and_cluster_duplicates(driver)\n",
    "        \n",
    "        if not clusters:\n",
    "            print(\"No node duplicates found.\")\n",
    "        else:\n",
    "            print(f\"Found {len(clusters)} groups of node duplicates.\")\n",
    "            for i, node_ids in enumerate(clusters):\n",
    "                print(f\"Processing Group {i+1} / {len(clusters)} (Size: {len(node_ids)})...\")\n",
    "                try:\n",
    "                    # A. Get data\n",
    "                    props_list = get_node_properties(driver, node_ids)\n",
    "                    \n",
    "                    # B. Determine best Vector (Most recent updated_at)\n",
    "                    # Use .get with default 0/empty to avoid errors if updated_at is missing\n",
    "                    sorted_props = sorted(props_list, key=lambda x: x.get('updated_at', ''))\n",
    "                    master_vector = sorted_props[-1].get('vector', None)\n",
    "                    \n",
    "                    # C. Generate Golden Record (Pass model explicitly)\n",
    "                    golden_record = generate_golden_record(model, props_list)\n",
    "                    \n",
    "                    # D. Merge in DB\n",
    "                    with driver.session() as session:\n",
    "                        session.write_transaction(execute_merge, node_ids, golden_record, master_vector)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error merging group {node_ids}: {e}\")\n",
    "    finally:\n",
    "        driver.close()\n",
    "\n",
    "# ==========================================\n",
    "# 3. EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Merge Duplicate Nodes\n",
    "    process_smart_deduplication()\n",
    "    \n",
    "    # 2. Cleanup Duplicate Edges (Nuclear Delete)\n",
    "    force_merge_edges_nuclear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning nodes for property value: 'AMF Fonder AB'...\n",
      "(This handles Lists/Arrays safely by searching inside Python)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7r/xtx89_9j54l4w8l78by0ycn80000gn/T/ipykernel_88584/3221313440.py:26: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  with driver.session() as session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MATCH #1 (ID: 4:b6ec1d34-ccf6-4be1-b199-4893023a567d:44) ---\n",
      "Labels: ['Fund']\n",
      "Match found in key(s): ['name']\n",
      "Properties:\n",
      "{\n",
      "    \"country_code\": \"SE\",\n",
      "    \"mission\": \"\",\n",
      "    \"website\": \"\",\n",
      "    \"sectors\": [],\n",
      "    \"aliases\": [\n",
      "        \"AMF Fonder\",\n",
      "        \"AMF Pension & Fonder\",\n",
      "        \"AMF Pension & Funds\"\n",
      "    ],\n",
      "    \"company_id\": \"556549-2922\",\n",
      "    \"updated_at\": \"2025-12-06T15:43:47.223000000+00:00\",\n",
      "    \"investment_thesis\": \"\",\n",
      "    \"year_founded\": \"1997\",\n",
      "    \"name\": \"AMF Fonder AB\",\n",
      "    \"description\": \"Ingested as shareholder of 556042-7220\",\n",
      "    \"key_people\": []\n",
      "}\n",
      "\n",
      "\n",
      "Search complete. Found 1 nodes.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "SEARCH_TERM = \"AMF Fonder AB\"\n",
    "\n",
    "def find_ghost_node_safe():\n",
    "    driver = get_driver()\n",
    "    \n",
    "    print(f\"Scanning nodes for property value: '{SEARCH_TERM}'...\")\n",
    "    print(\"(This handles Lists/Arrays safely by searching inside Python)\\n\")\n",
    "    \n",
    "    # 1. Fetch ALL properties for nodes that might be relevant\n",
    "    # We limit to nodes that actually have some properties to check\n",
    "    query = \"\"\"\n",
    "    MATCH (n)\n",
    "    WHERE keys(n) IS NOT NULL\n",
    "    RETURN \n",
    "        elementId(n) as id, \n",
    "        labels(n) as labels, \n",
    "        properties(n) as all_props\n",
    "    LIMIT 50000 \n",
    "    \"\"\"\n",
    "    \n",
    "    found_count = 0\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        result = session.run(query)\n",
    "        \n",
    "        # 2. Iterate in Python (Type-Safe)\n",
    "        for record in result:\n",
    "            props = record[\"all_props\"]\n",
    "            node_id = record[\"id\"]\n",
    "            labels = record[\"labels\"]\n",
    "            \n",
    "            # Check if search term exists in ANY value of the dictionary\n",
    "            # We convert values to str() here in Python, which handles Lists/Ints/None perfectly\n",
    "            match_found = False\n",
    "            matching_keys = []\n",
    "            \n",
    "            for key, value in props.items():\n",
    "                if SEARCH_TERM in str(value):\n",
    "                    match_found = True\n",
    "                    matching_keys.append(key)\n",
    "            \n",
    "            if match_found:\n",
    "                found_count += 1\n",
    "                print(f\"--- MATCH #{found_count} (ID: {node_id}) ---\")\n",
    "                print(f\"Labels: {labels}\")\n",
    "                print(f\"Match found in key(s): {matching_keys}\")\n",
    "                print(\"Properties:\")\n",
    "                print(json.dumps(props, indent=4, default=str))\n",
    "                print(\"\\n\")\n",
    "\n",
    "    print(f\"Search complete. Found {found_count} nodes.\")\n",
    "    driver.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_ghost_node_safe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_ID = \"556789-1234\" \n",
    "\n",
    "def find_node_by_id_safe():\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    \n",
    "    print(f\"Scanning nodes for ID value: '{SEARCH_ID}'...\")\n",
    "    \n",
    "    # We fetch ALL nodes (limited) to ensure we don't miss any due to label filtering\n",
    "    query = \"\"\"\n",
    "    MATCH (n)\n",
    "    WHERE keys(n) IS NOT NULL\n",
    "    RETURN \n",
    "        elementId(n) as id, \n",
    "        labels(n) as labels, \n",
    "        properties(n) as all_props\n",
    "    LIMIT 50000 \n",
    "    \"\"\"\n",
    "    \n",
    "    found_count = 0\n",
    "    target_str = str(SEARCH_ID).strip()\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        result = session.run(query)\n",
    "        \n",
    "        for record in result:\n",
    "            props = record[\"all_props\"]\n",
    "            node_id = record[\"id\"]\n",
    "            labels = record[\"labels\"]\n",
    "            \n",
    "            match_found = False\n",
    "            matching_keys = []\n",
    "            \n",
    "            # Iterate over every property key to find the ID\n",
    "            for key, value in props.items():\n",
    "                # Convert DB value to string and strip whitespace for comparison\n",
    "                # This handles cases where DB has 12345 (int) vs \"12345\" (str)\n",
    "                if str(value).strip() == target_str:\n",
    "                    match_found = True\n",
    "                    matching_keys.append(key)\n",
    "            \n",
    "            if match_found:\n",
    "                found_count += 1\n",
    "                print(f\"--- MATCH #{found_count} (Node ID: {node_id}) ---\")\n",
    "                print(f\"Labels: {labels}\")\n",
    "                print(f\"ID found in property key(s): {matching_keys}\")\n",
    "                print(\"Properties:\")\n",
    "                print(json.dumps(props, indent=4, default=str))\n",
    "                print(\"\\n\")\n",
    "\n",
    "    print(f\"Search complete. Found {found_count} nodes.\")\n",
    "    driver.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_node_by_id_safe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' org_id = structured_data.get(\\'organization_id\\')\\nprint(f\"Extracted Organization ID: {org_id}\")\\nprint(\"1Ô∏è‚É£  Authenticating...\")\\n# Call the Bolagsverket API to get more informaiton on the company \\ntoken = get_access_token()\\nif token:\\n    print(\"‚úÖ Access Token received!\")\\n\\n    # Example: Search for Bolagsverket\\'s own org number (202100-5489)\\n    # Remove hyphen for the API: 2021005489\\n    test_org_number = 9697802230\\n\\n    print(f\"2Ô∏è‚É£  Searching for company: {test_org_number}...\")\\n    boglagsverket_api_data = search_company(test_org_number, token)\\n\\n    if boglagsverket_api_data:\\n        print(\"‚úÖ Data received:\")\\n        print(boglagsverket_api_data)\\nelse:\\n    print(\"üõë Could not proceed without token.\") '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" org_id = structured_data.get('organization_id')\n",
    "print(f\"Extracted Organization ID: {org_id}\")\n",
    "print(\"1Ô∏è‚É£  Authenticating...\")\n",
    "# Call the Bolagsverket API to get more informaiton on the company \n",
    "token = get_access_token()\n",
    "if token:\n",
    "    print(\"‚úÖ Access Token received!\")\n",
    "    \n",
    "    # Example: Search for Bolagsverket's own org number (202100-5489)\n",
    "    # Remove hyphen for the API: 2021005489\n",
    "    test_org_number = 9697802230\n",
    "    \n",
    "    print(f\"2Ô∏è‚É£  Searching for company: {test_org_number}...\")\n",
    "    boglagsverket_api_data = search_company(test_org_number, token)\n",
    "    \n",
    "    if boglagsverket_api_data:\n",
    "        print(\"‚úÖ Data received:\")\n",
    "        print(boglagsverket_api_data)\n",
    "else:\n",
    "    print(\"üõë Could not proceed without token.\") \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
